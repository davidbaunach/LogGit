<!DOCTYPE HTML>
<html lang="en">
<p>Saving copies of everything is like low-budget p2p</p>
<p>archived: 2-8-2024 from https://subconscious.substack.com/p/saving-copies-of-everything-is-like</p>
<p>100 Rabbits is the practice of two creatives who write, illustrate, compose, and build software while living on a sailboat.</p>
<p>Working out of a nomadic floating studio presents unusual challenges. Work schedules must change with the weather, since computers are solar powered. Internet connections are sporadic—a fast connection at a port or cafe, no connection at sea. So they get creative:</p>
<p> When we have a reliable internet connection, we gather copies of all the online material we will need. We keep offline versions of entire websites, writing guides, articles and even whole sections of Wikipedia (using wget) If we find ourselves without a connection, we can still solve our problems by using our offline mirrors. By the way, you can download our entire website.</p>
<p>_100 Rabbits, “Working offgrid efficiently”_</p>
<p>This is what IDEO might call an extreme user—someone who has a set of needs outside of the mainstream. When you encounter an extreme user, pay attention:</p>
<p>-   Extreme users reveal needs, use-cases, opportunities that everyone might have, but are invisible within mainstream behavior.</p>
<p>-   Extreme users can be harbingers, part of a future that is already here, but not evenly distributed yet.</p>
<p>-   Extreme users push the boundaries of technologies. “The street finds its own uses for things”. Different contexts generate new use-cases and fits—lateral thinking with withered technology—inspiring hacks, modifications, and riffs with established building blocks.</p>
<p>-   Solutions for extreme users will usually work for mainstream users, often better than mainstream designs.</p>
<p>So, what new ideas are revealed by working on a sailboat? Following 100 Rabbits, I started archiving useful web pages with wget, as an experiment. A few sparks…</p>
<p>**Saving pages with wget is like low-budget p2p**. It’s interesting that you can get many of the benefits of p2p just by saving the things you pull down with http. You get your own little copy of the page, stashed away in a folder. It doesn’t matter if the website goes down. You have the bits. You can take them with you. You can even re-host them. You have agency over the data, and the ability to share it anywhere, through any mechanism.</p>
<p>**Own a copy of your corner of the internet**. This shift in perspective from network-first, to local-first is compelling. It’s more resilient. It’s **more convivial**. It makes me want to re-imagine all kinds of things about the web:</p>
<p>-   What if the browser was local-first?</p>
<p>-   What if websites showed up as files and folders on my computer?</p>
<p>-   What if the browser saved a local copy of everything I bookmark?</p>
<p>-   What if I had my own personal wayback machine?</p>
<p>-   What if I had a little local Google that could search the full text of everything I’ve ever saved?</p>
<p>-   What if I could open website files, edit, and remix them? Add links. Mark them up with highlights Write margin notes.</p>
<p>-   What if the whole web was built around copying/remixing/sharing?</p>
<p>Much of this is possible to cobble together with the systems we have today: wget, http, web pages. It’s not perfect, but it works.</p>
<p>Such a local-first web might also fare better in a climate-changed future where always-on internet might not be something we can take for granted.</p>
